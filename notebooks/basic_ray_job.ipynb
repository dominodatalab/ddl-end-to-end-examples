{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a54bc1-58ed-42ae-ae0c-341a0ca2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336f048-16eb-441b-9c12-74309c613578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost as mlflow_xgb\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data import read_parquet\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "try:\n",
    "    from ray.tune.callback import Callback      # Ray >= 2.6\n",
    "except ImportError:\n",
    "    from ray.tune.callbacks import Callback     # Older Ray\n",
    "from utils import ddl_cluster_scaling_client\n",
    "from utils import mlflow_utils\n",
    "from utils import ray_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe473df-6ff9-4cce-8bbf-10f4493cb6dd",
   "metadata": {},
   "source": [
    "## Pre-requsites\n",
    "\n",
    "Configure the following user environment variables\n",
    "\n",
    "1. AWS_ROLE_ARN - This is the AWS role being assumed via IR\n",
    "2. S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af9bda-73fe-4dde-b2b5-c9c6e54ee9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset and push to S3\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.rename(columns={\"MedHouseVal\": \"median_house_value\"})\n",
    "\n",
    "# Split\n",
    "train, tmp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test  = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save locally\n",
    "train.to_parquet(\"/tmp/train.parquet\", index=False)\n",
    "val.to_parquet(\"/tmp/val.parquet\", index=False)\n",
    "test.to_parquet(\"/tmp/test.parquet\", index=False)\n",
    "\n",
    "# Push to S3\n",
    "!aws s3 cp /tmp/train.parquet s3://${S3_BUCKET_NAME}/end-to-end/california/train/\n",
    "!aws s3 cp /tmp/val.parquet   s3://${S3_BUCKET_NAME}/end-to-end/california/val/\n",
    "!aws s3 cp /tmp/test.parquet  s3://${S3_BUCKET_NAME}/end-to-end/california/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7040bd1-cd6e-4be0-98d9-2f90d4a527ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adda697-3841-450e-98f4-bed31db79159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _s3p(root: str, sub: str) -> str:\n",
    "    \"\"\"Safe join for S3/posix URIs.\"\"\"\n",
    "    return f\"{root.rstrip('/')}/{sub.lstrip('/')}\"\n",
    "\n",
    "\n",
    "def read_parquet_to_pandas(uri: str, columns=None, limit: int | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust Parquetâ†’pandas loader that bypasses Ray Data.\n",
    "    Works with local paths and s3:// (PyArrow uses AWS_* env vars / IRSA).\n",
    "    \"\"\"\n",
    "    ds = pds.dataset(uri.rstrip(\"/\"), format=\"parquet\")\n",
    "    if limit is None:\n",
    "        return ds.to_table(columns=columns).to_pandas()\n",
    "\n",
    "    # Respect limit across files/row groups\n",
    "    scanner = pds.Scanner.from_dataset(ds, columns=columns)\n",
    "    batches, rows = [], 0\n",
    "    for b in scanner.to_batches():\n",
    "        batches.append(b)\n",
    "        rows += len(b)\n",
    "        if rows >= limit:\n",
    "            return pa.Table.from_batches(batches)[:limit].to_pandas()\n",
    "    return pa.Table.from_batches(batches).to_pandas()\n",
    "\n",
    "\n",
    "def main(experiment_name:str,data_dir: str,\n",
    "         model_name:str,model_desc:str,\n",
    "         num_workers: int = 4, cpus_per_worker: int = 1,  DEV_FAST: bool = False):\n",
    "    \"\"\"\n",
    "    Quick knobs:\n",
    "      - num_workers * cpus_per_worker = CPUs per trial.\n",
    "      - trainer_resources={\"CPU\":0} so the driver doesn't steal a core.\n",
    "      - PACK placement to keep trials tight.\n",
    "      - max_concurrent_trials caps parallel trials.\n",
    "      - num_boost_round / early_stopping_rounds control trial length.\n",
    "      - nthread = cpus_per_worker to avoid oversubscription.\n",
    "    \"\"\"\n",
    "\n",
    "    exp_id = mlflow_utils.ensure_mlflow_experiment(experiment_name)\n",
    "    mv = mlflow_utils.ensure_registered_model(model_name)\n",
    "    # Storage: local for dev, S3/your env otherwise\n",
    "    RUN_STORAGE = os.environ.get(\"RAY_AIR_STORAGE\", f\"{data_dir}/air/xgb\")\n",
    "    TUNER_STORAGE = \"/tmp/air-dev\" if DEV_FAST else RUN_STORAGE\n",
    "    FINAL_STORAGE = \"/mnt/data/ddl-end-to-end-demo/air/final_fit\" if DEV_FAST else RUN_STORAGE\n",
    "\n",
    "    # Sanity: workers see IRSA env?\n",
    "    @ray.remote\n",
    "    def _peek():\n",
    "        import os\n",
    "        return {\n",
    "            \"ROLE\": bool(os.environ.get(\"AWS_ROLE_ARN\")),\n",
    "            \"TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\"),\n",
    "            \"REGION\": os.environ.get(\"AWS_REGION\"),\n",
    "        }\n",
    "    print(\"Worker env peek:\", ray.get(_peek.remote()))\n",
    "\n",
    "    # MLflow (experiment + parent run)\n",
    "    CLUSTER_TRACKING_URI = os.environ[\"CLUSTER_MLFLOW_TRACKING_URI\"]\n",
    "    \n",
    "    \n",
    "    client = MlflowClient()\n",
    "\n",
    "\n",
    "    parent = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        tags={\"mlflow.runName\": \"xgb_parent\", \"role\": \"tune_parent\"},\n",
    "    )\n",
    "    parent_run_id = parent.info.run_id\n",
    "    print(\"Parent run id:\", parent_run_id)\n",
    "\n",
    "    # Data (Ray Datasets for training/val)\n",
    "    train_ds = read_parquet(_s3p(data_dir, \"train\"), parallelism=num_workers)\n",
    "    val_ds   = read_parquet(_s3p(data_dir, \"val\"),   parallelism=num_workers)\n",
    "    test_ds  = read_parquet(_s3p(data_dir, \"test\"),  parallelism=num_workers)\n",
    "    print(\"Schema:\", train_ds.schema())\n",
    "\n",
    "    # Label + features\n",
    "    label_col = \"median_house_value\"\n",
    "    feature_cols = [c for c in train_ds.schema().names if c != label_col]\n",
    "    keep = feature_cols + [label_col]\n",
    "    train_ds = train_ds.select_columns(keep)\n",
    "    val_ds   = val_ds.select_columns(keep)\n",
    "\n",
    "    # DEV: trim Ray Datasets used for training; eval will bypass Ray entirely\n",
    "    if DEV_FAST:\n",
    "        train_ds = train_ds.limit(5_000)\n",
    "        val_ds   = val_ds.limit(2_000)\n",
    "\n",
    "    # --- Build test DataFrame without Ray (avoids 'Global node is not initialized') ---\n",
    "    test_uri = _s3p(data_dir, \"test\")\n",
    "    test_pdf = read_parquet_to_pandas(\n",
    "        test_uri, columns=keep, limit=2_000 if DEV_FAST else None\n",
    "    )\n",
    "\n",
    "    # Search space\n",
    "    param_space = {\n",
    "        \"params\": {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"eta\": tune.loguniform(1e-3, 3e-1),\n",
    "            \"max_depth\": tune.randint(4, 12),\n",
    "            \"min_child_weight\": tune.loguniform(1e-2, 10),\n",
    "            \"subsample\": tune.uniform(0.6, 1.0),\n",
    "            \"colsample_bytree\": tune.uniform(0.6, 1.0),\n",
    "            \"lambda\": tune.loguniform(1e-3, 10),\n",
    "            \"alpha\": tune.loguniform(1e-3, 10),\n",
    "        },\n",
    "        \"num_boost_round\": 300,\n",
    "        \"early_stopping_rounds\": 20,\n",
    "    }\n",
    "\n",
    "    # Dev shortcuts\n",
    "    if DEV_FAST:\n",
    "        param_space[\"num_boost_round\"] = 20\n",
    "        param_space[\"early_stopping_rounds\"] = 5\n",
    "        num_workers = 1\n",
    "        cpus_per_worker = 1\n",
    "        NUM_SAMPLES = 5\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "    else:\n",
    "        NUM_SAMPLES = 30\n",
    "        MAX_CONCURRENT = 3\n",
    "        SAVE_ARTIFACTS = True\n",
    "\n",
    "    # Threads per worker\n",
    "    param_space[\"params\"][\"nthread\"] = cpus_per_worker\n",
    "    print(\"Per-trial CPUs =\", num_workers * cpus_per_worker)\n",
    "\n",
    "    # Scaling / placement\n",
    "    scaling = ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": cpus_per_worker},\n",
    "        trainer_resources={\"CPU\": 0},\n",
    "        placement_strategy=\"PACK\",\n",
    "    )\n",
    "\n",
    "    # Trainable\n",
    "    trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=param_space[\"params\"],\n",
    "        datasets={\"train\": train_ds, \"valid\": val_ds},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "    )\n",
    "\n",
    "    # Search + scheduler\n",
    "    MAX_T = int(param_space[\"num_boost_round\"])\n",
    "    GRACE = int(min(param_space.get(\"early_stopping_rounds\", 1), MAX_T))\n",
    "    algo = HyperOptSearch(metric=\"valid-rmse\", mode=\"min\")\n",
    "    scheduler = ASHAScheduler(max_t=MAX_T, grace_period=GRACE, reduction_factor=3)\n",
    "\n",
    "    # MLflow callback (child runs)\n",
    "    mlflow_cb = MLflowLoggerCallback(\n",
    "        tracking_uri=CLUSTER_TRACKING_URI,\n",
    "        experiment_name=experiment_name,\n",
    "        save_artifact=SAVE_ARTIFACTS,\n",
    "        tags={\"mlflow.parentRunId\": parent_run_id},\n",
    "    )\n",
    "\n",
    "    # Tuner\n",
    "    tuner = tune.Tuner(\n",
    "        trainer.as_trainable(),\n",
    "        run_config=RunConfig(\n",
    "            name=\"xgb_from_s3_irsa\",\n",
    "            storage_path=TUNER_STORAGE,\n",
    "            callbacks=[mlflow_cb],\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=algo,\n",
    "            scheduler=scheduler,\n",
    "            metric=\"valid-rmse\",\n",
    "            mode=\"min\",\n",
    "            num_samples=NUM_SAMPLES,\n",
    "            max_concurrent_trials=MAX_CONCURRENT,\n",
    "        ),\n",
    "        param_space={\"params\": param_space[\"params\"]},\n",
    "    )\n",
    "\n",
    "    # Tune\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"valid-rmse\", mode=\"min\")\n",
    "    print(\"Best config:\", best.config)\n",
    "    print(\"Best valid RMSE:\", best.metrics.get(\"valid-rmse\"))\n",
    "\n",
    "    # Final fit (train + val)\n",
    "    merged = train_ds.union(val_ds)\n",
    "    final_trainer = XGBoostTrainer(\n",
    "        label_column=label_col,\n",
    "        params=best.config[\"params\"],\n",
    "        datasets={\"train\": merged},\n",
    "        num_boost_round=param_space[\"num_boost_round\"],\n",
    "        scaling_config=scaling,\n",
    "        run_config=RunConfig(name=\"final_fit\", storage_path=FINAL_STORAGE),\n",
    "    )\n",
    "    final_result = final_trainer.fit()\n",
    "    final_ckpt = final_result.checkpoint\n",
    "\n",
    "    # Load Booster from checkpoint\n",
    "    with final_ckpt.as_directory() as ckpt_dir:\n",
    "        print(\"Checkpoint dir:\", ckpt_dir, \"files:\", os.listdir(ckpt_dir))\n",
    "        candidates = [\"model.json\", \"model.ubj\", \"model.xgb\", \"xgboost_model.json\", \"model\"]\n",
    "        model_path = next(\n",
    "            (os.path.join(ckpt_dir, f) for f in candidates if os.path.exists(os.path.join(ckpt_dir, f))),\n",
    "            None,\n",
    "        )\n",
    "        if not model_path:\n",
    "            raise FileNotFoundError(f\"No XGBoost model file found in checkpoint dir: {ckpt_dir}\")\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(model_path)\n",
    "\n",
    "    # Driver-side eval (no Ray dependency)\n",
    "    X_test = test_pdf.drop(columns=[label_col])\n",
    "    \n",
    "    dmat = xgb.DMatrix(X_test)\n",
    "    y_pred = booster.predict(dmat)\n",
    "    rmse = math.sqrt(((test_pdf[label_col].to_numpy() - y_pred) ** 2).mean())\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # Log final under parent\n",
    "\n",
    "    with mlflow.start_run(run_id=parent_run_id):\n",
    "        X_example = X_test.head(5).copy()  \n",
    "        y_example = booster.predict(xgb.DMatrix(X_example))\n",
    "        sig = infer_signature(X_example, y_example)\n",
    "        with mlflow.start_run(run_name=\"final_fit\", nested=True) as final_run:\n",
    "            mlflow.log_params(best.config.get(\"params\", {}))\n",
    "            mlflow.log_dict({\"label\": label_col, \"features\": feature_cols}, \"features.json\")\n",
    "            mlflow.log_metric(\"valid_rmse_best\", float(best.metrics.get(\"valid-rmse\")))\n",
    "            mlflow.log_metric(\"test_rmse\", float(rmse))\n",
    "            model_info = mlflow_xgb.log_model(booster, artifact_path=\"model\",signature=sig,input_example=X_example)\n",
    "\n",
    "            mv = mlflow_utils.register_model_version(model_name=model_name,model_desc=model_desc,\n",
    "                                                model_info=model_info,run=final_run)\n",
    "            \n",
    "            print(\"Name: {}\".format(mv.name))\n",
    "            print(\"Version: {}\".format(mv.version))\n",
    "            print(\"Description: {}\".format(mv.description))\n",
    "            print(\"Status: {}\".format(mv.status))\n",
    "            print(\"Stage: {}\".format(mv.current_stage))\n",
    "            \n",
    "    \n",
    "    run = client.get_run(parent_run_id)\n",
    "    if run.info.status == \"RUNNING\":\n",
    "        client.set_terminated(parent_run_id, \"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee926e-ca00-486d-af2c-53cd603c4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_cluster_scaling_client.scale_cluster(cluster_kind=\"rayclusters\",worker_hw_tier_name=\"Medium\", replicas=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55bb35-622c-48cc-8204-fc219791c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_cluster_scaling_client.wait_until_scaling_complete(cluster_kind=\"rayclusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab02e5-1bff-4223-a424-8a1a78496bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ddl_cluster_scaling_client\n",
    "j = ddl_cluster_scaling_client.restart_head_node(cluster_kind=\"rayclusters\")\n",
    "restarts_at = j['started_at']\n",
    "print(restarts_at)\n",
    "ddl_cluster_scaling_client.restart_head_node_status(cluster_kind=\"rayclusters\",restarted_since=restarts_at)\n",
    "ddl_cluster_scaling_client.wait_until_node_restarted(cluster_kind=\"rayclusters\",restarted_since=restarts_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0c61b-69d8-4686-858b-67ad14a2d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Conf from Hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "# Point Hydra to your conf/ directory\n",
    "with initialize(config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[\"env=dev\"])\n",
    "    #print(f\"Running in {cfg.env} environment\")\n",
    "    #print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "    \n",
    "    app_name = cfg.app.name\n",
    "    data_dir = cfg.app.data_dir\n",
    "    experiment_name = cfg.mlflow.experiment_name    \n",
    "    model_name = cfg.mlflow.model_name    \n",
    "    model_desc = cfg.mlflow.model_desc\n",
    "    ray_workers = cfg.env.ray.num_workers\n",
    "    cpus_per_worker = cfg.env.ray.cpus_per_worker\n",
    "    dev_fast = cfg.env.ray.dev_fast\n",
    "    #print(ray_workers)\n",
    "    #print(dev_fast)\n",
    "    \n",
    "# Disable tensorboard integration\n",
    "os.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ba8ff-d2f0-4ad2-83b8-501b4dc41640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RAY_JOB_ENV = {\n",
    "    \"AWS_ROLE_ARN\": os.environ.get(\"AWS_ROLE_ARN\", \"\"),\n",
    "    \"AWS_WEB_IDENTITY_TOKEN_FILE\": os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\", \"\"),\n",
    "    \"AWS_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")),\n",
    "    \"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\":\"1\",\n",
    "    \"TUNE_RESULT_BUFFER_LENGTH\": \"16\",\n",
    "    \"TUNE_RESULT_BUFFER_FLUSH_INTERVAL_S\": \"3\",    \n",
    "    \n",
    "}\n",
    "ray.shutdown()\n",
    "ray_utils.ensure_ray_connected(RAY_JOB_ENV,ray_ns=app_name)\n",
    "\n",
    "main(experiment_name=experiment_name,data_dir=data_dir, \n",
    "     model_name=model_name,model_desc=model_desc,\n",
    "     num_workers=4, cpus_per_worker=1,DEV_FAST=dev_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6275c8-9ca7-4900-ab60-de5a420e82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ddl_cluster_scaling_client\n",
    "ddl_cluster_scaling_client.scale_cluster(cluster_kind=\"rayclusters\",worker_hw_tier_name=\"Small\",replicas=1)\n",
    "ddl_cluster_scaling_client.wait_until_scaling_complete(cluster_kind=\"rayclusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7b117-675f-450b-8076-fcef8bf7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mlflow_utils\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "\n",
    "my_model = mlflow_utils.load_registered_model_version(model_name,\"latest\")\n",
    "\n",
    "# your split-style payload\n",
    "split = {\n",
    "  \"columns\": [\"MedInc\",\"HouseAge\",\"AveRooms\",\"AveBedrms\",\"Population\",\"AveOccup\",\"Latitude\",\"Longitude\"],\n",
    "  \"data\": [\n",
    "    [3.1333,30.0,5.925531914893617,1.1312056737588652,966.0,3.425531914893617,36.51,-119.65],\n",
    "    [2.3355,18.0,5.711722488038277,1.0598086124401913,1868.0,2.2344497607655502,33.97,-117.01],\n",
    "    [3.3669,29.0,4.5898778359511345,1.0767888307155322,1071.0,1.869109947643979,34.15,-118.37],\n",
    "    [3.875,46.0,4.0,1.0,59.0,4.538461538461538,33.12,-117.11],\n",
    "    [4.3482,9.0,5.7924528301886795,1.1037735849056605,409.0,1.929245283018868,35.36,-119.06]\n",
    "  ]\n",
    "}\n",
    "\n",
    "# make a DataFrame\n",
    "X = pd.DataFrame(split[\"data\"], columns=split[\"columns\"])\n",
    "preds = my_model.predict(X)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e00d6-0f97-4180-a994-7cb7fc0e83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, sys, pyarrow as pa, pandas as pd\n",
    "print(\"DRIVER:\", sys.version)\n",
    "print(\"DRIVER pyarrow:\", pa.__version__)\n",
    "print(\"DRIVER pandas :\", pd.__version__)\n",
    "\n",
    "@ray.remote\n",
    "def _env_probe():\n",
    "    import sys, pyarrow as pa, pandas as pd\n",
    "    return {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pyarrow\": pa.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "    }\n",
    "\n",
    "print(\"WORKER:\", ray.get(_env_probe.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a23211-b6aa-469e-878f-b19ec0943134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd3a8b-6669-45fa-9368-dc722d74746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
