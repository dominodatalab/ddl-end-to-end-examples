{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dad88-ff5c-478d-b5bc-db819d934859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44efccb-519c-4125-96b3-4bcc95d0f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,socket,json, time\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59470a39-30ab-491c-989d-4684d3b57f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(\"pyspark:\", pyspark.__version__)  # e.g. 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ff366-c4e0-4b07-beb0-ad826c1e56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This needs a bucket level  actions [\"s3:ListBucket\",\"s3:GetBucketLocation\"],\n",
    "def delete_if_exists(s3_uri: str):\n",
    "    if not s3_uri.startswith(\"s3://\"):\n",
    "        raise ValueError(\"Path must start with s3://\")\n",
    "\n",
    "    # split into bucket + key\n",
    "    _, _, bucket_and_key = s3_uri.partition(\"s3://\")\n",
    "    bucket, _, key = bucket_and_key.partition(\"/\")\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    try:\n",
    "        # check if object exists\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            print(f\"{s3_uri} does not exist, nothing to delete.\")\n",
    "            return\n",
    "        else:\n",
    "            raise\n",
    "    # delete if it exists\n",
    "    s3.delete_object(Bucket=bucket, Key=key)\n",
    "    print(f\"Deleted {s3_uri}\")\n",
    "\n",
    "## This deletes without bucket level actions\n",
    "def delete_object_idempotent(uri: str):\n",
    "    _, _, rest = uri.partition(\"s3://\")\n",
    "    bucket, _, key = rest.partition(\"/\")\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3.delete_object(Bucket=bucket, Key=key)\n",
    "        print(f\"Delete requested: s3://{bucket}/{key}\")\n",
    "    except ClientError as e:\n",
    "        print(\"Delete failed:\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba30de-6ea5-422c-b678-3c7bc5a0cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def _log_from_workers(partition_iter):\n",
    "    \"\"\"Runs on executors; prints IRSA/env visibility.\"\"\"\n",
    "    role = os.environ.get(\"AWS_ROLE_ARN\", \"<missing>\")\n",
    "    tok  = os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\", \"<missing>\")\n",
    "    akid = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"<missing>\")\n",
    "    host = socket.gethostname()\n",
    "    print(f\"[EXECUTOR {host}] AWS_ROLE_ARN={role}\")\n",
    "    print(f\"[EXECUTOR {host}] AWS_WEB_IDENTITY_TOKEN_FILE={tok}\")\n",
    "    print(f\"[EXECUTOR {host}] AWS_ACCESS_KEY_ID set? {'yes' if akid!='<missing>' else 'no'}\")\n",
    "    for row in partition_iter:\n",
    "        yield row\n",
    "\n",
    "def create_synthetic_text(spark: SparkSession, s3_text_path: str, n_lines: int = 1000, seed: int = 42):\n",
    "    \"\"\"Create a 1-col DF ('value') and write it as TEXT to s3a://...\"\"\"\n",
    "    df = (\n",
    "        spark.range(0, n_lines, 1, numPartitions=max(1, min(32, n_lines // 100)))\n",
    "             .withColumn(\"value\", F.concat_ws(\n",
    "                 \" \",\n",
    "                 F.lit(\"line\"),\n",
    "                 F.col(\"id\").cast(\"string\"),\n",
    "                 F.lit(\"payload\"),\n",
    "                 F.sha2(F.concat(F.col(\"id\").cast(\"string\"), F.lit(str(seed))), 256)\n",
    "             ))\n",
    "             .select(\"value\")\n",
    "    )\n",
    "    df.write.mode(\"overwrite\").text(s3_text_path)\n",
    "    print(f\"Wrote synthetic text ({n_lines} lines) to: {s3_text_path}\")\n",
    "\n",
    "'''\n",
    "When the JVM is already up, it won’t see new env vars (Java reads env once at process start). Your STS creds end up in Python, but S3A keeps using the old, empty provider chain.\n",
    "\n",
    "Two ways out. Since restarting the kernel is annoying, here’s a patch-in-place route that works even with an existing session:\n",
    "1. Fix without restarting: inject STS creds into Hadoop conf and flush S3A cache\n",
    "    Drop this helper right after you call assume_with_web_identity() and before any s3a:// read/write:\n",
    "2.Before building the session, stop the old one so your .config(\"spark.jars.packages\", ...) and IRSA settings actually load:\n",
    "try:\n",
    "    SparkSession.getActiveSession().stop()\n",
    "except Exception:\n",
    "    pass\n",
    "Then build a fresh session and proceed (your earlier “fresh-session” version is fine). But in Domino/managed notebooks, sometimes a platform daemon immediately recreates \n",
    "a baseline session; when that happens, the Hadoop-conf injection above is the reliable move.\n",
    "'''\n",
    "def apply_s3a_temp_creds_to_hadoop(spark, creds: dict, region: str = None):\n",
    "    \"\"\"\n",
    "    Inject temporary AWS creds directly into Hadoop config so S3A uses them,\n",
    "    even if the Spark JVM/session was already running.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        region = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "    # Tell S3A to use temp session creds (AKIA + SECRET + TOKEN)\n",
    "    hconf.set(\"fs.s3a.access.key\", creds[\"AWS_ACCESS_KEY_ID\"])\n",
    "    hconf.set(\"fs.s3a.secret.key\", creds[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "    hconf.set(\"fs.s3a.session.token\", creds[\"AWS_SESSION_TOKEN\"])\n",
    "    hconf.set(\"fs.s3a.aws.region\", region)\n",
    "\n",
    "    # Make the provider selection unambiguous\n",
    "    # (TemporaryAWSCredentialsProvider is used when session.token is present,\n",
    "    #  but we set it explicitly to avoid the wrong chain.)\n",
    "    hconf.set(\"fs.s3a.aws.credentials.provider\",\n",
    "              \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "\n",
    "    # Safety: ensure S3A impl is present + no stale cached FS is reused\n",
    "    hconf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    hconf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    hconf.set(\"fs.s3a.impl.disable.cache\", \"true\")\n",
    "\n",
    "    # Drop any cached FileSystem instances so the new creds take effect\n",
    "    jvm = spark._jvm\n",
    "    jvm.org.apache.hadoop.fs.FileSystem.closeAll()\n",
    "\n",
    "\n",
    "def assume_with_web_identity():\n",
    "    \"\"\"Use IRSA (web identity) to obtain short-lived env creds via STS.\"\"\"\n",
    "    role_arn = os.environ[\"AWS_ROLE_ARN\"]\n",
    "    token_fn = os.environ[\"AWS_WEB_IDENTITY_TOKEN_FILE\"]\n",
    "    session_name = os.environ.get(\"AWS_ROLE_SESSION_NAME\", \"spark-irsa\")\n",
    "    region = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "    with open(token_fn, \"r\") as f:\n",
    "        jwt = f.read()\n",
    "\n",
    "    sts = boto3.client(\"sts\", region_name=region)\n",
    "    resp = sts.assume_role_with_web_identity(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=session_name,\n",
    "        WebIdentityToken=jwt,\n",
    "        DurationSeconds=3600,\n",
    "    )\n",
    "    c = resp[\"Credentials\"]\n",
    "    creds = {\n",
    "        \"AWS_ACCESS_KEY_ID\": c[\"AccessKeyId\"],\n",
    "        \"AWS_SECRET_ACCESS_KEY\": c[\"SecretAccessKey\"],\n",
    "        \"AWS_SESSION_TOKEN\": c[\"SessionToken\"],\n",
    "        \"AWS_REGION\": region,\n",
    "    }\n",
    "    os.environ.update(creds)  # driver process sees them too\n",
    "    return creds\n",
    "\n",
    "def to_s3a(u: str) -> str:\n",
    "    return u.replace(\"s3://\", \"s3a://\", 1) if u and u.startswith(\"s3://\") else u\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main(s3_input_text: str, s3_output_parquet: str = None, make_synth: bool = False, synth_lines: int = 1000):\n",
    "    # Fresh session (avoid \"Using an existing Spark session\" which ignores new jars/config)\n",
    "    try:\n",
    "        # Stop any existing session to ensure classpath + provider configs apply\n",
    "        spark  # type: ignore\n",
    "        SparkSession.getActiveSession().stop()  # type: ignore\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 1) Bootstrap short-lived env creds from IRSA\n",
    "    creds = assume_with_web_identity()\n",
    "\n",
    "    s3a_input  = to_s3a(s3_input_text)\n",
    "    s3a_output = to_s3a(s3_output_parquet) if s3_output_parquet else None\n",
    "    region     = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "    if SparkSession.getActiveSession():\n",
    "        SparkSession.getActiveSession().stop()\n",
    "    # 2) Build Spark – pass temp env creds to driver & executors    \n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"basic-s3a-dataframe\")\n",
    "        # If your image lacks S3A, fetch jars (match Hadoop minor); if outbound blocked, replace with spark.jars=/path1,/path2\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.610\")\n",
    "        # Propagate short-lived creds (works with EnvironmentVariableCredentialsProvider)\n",
    "        .config(\"spark.driverEnv.AWS_ACCESS_KEY_ID\", creds[\"AWS_ACCESS_KEY_ID\"])\n",
    "        .config(\"spark.driverEnv.AWS_SECRET_ACCESS_KEY\", creds[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "        .config(\"spark.driverEnv.AWS_SESSION_TOKEN\", creds[\"AWS_SESSION_TOKEN\"])\n",
    "        .config(\"spark.executorEnv.AWS_ACCESS_KEY_ID\", creds[\"AWS_ACCESS_KEY_ID\"])\n",
    "        .config(\"spark.executorEnv.AWS_SECRET_ACCESS_KEY\", creds[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "        .config(\"spark.executorEnv.AWS_SESSION_TOKEN\", creds[\"AWS_SESSION_TOKEN\"])\n",
    "        # S3A basics\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.region\", region)\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.sdk.disableEc2Metadata=true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    apply_s3a_temp_creds_to_hadoop(spark, creds)\n",
    "    # Driver-side sanity\n",
    "    print(\"\\n=== DRIVER ENV CHECK ===\")\n",
    "    print(\"AWS_ROLE_ARN:\", os.environ.get(\"AWS_ROLE_ARN\"))\n",
    "    print(\"AWS_WEB_IDENTITY_TOKEN_FILE:\", os.environ.get(\"AWS_WEB_IDENTITY_TOKEN_FILE\"))\n",
    "    print(\"AWS_ACCESS_KEY_ID set?:\", \"yes\" if os.environ.get(\"AWS_ACCESS_KEY_ID\") else \"no\")\n",
    "    print(\"fs.s3a.impl:\", spark._jsc.hadoopConfiguration().get(\"fs.s3a.impl\"))\n",
    "    print(\"=====================================\\n\")\n",
    "\n",
    "    # 3) Create synthetic input (optional)\n",
    "    if make_synth:\n",
    "        create_synthetic_text(spark, s3a_input, n_lines=synth_lines)\n",
    "\n",
    "    # 4) Read text from S3, fan out to executors (logs IRSA/env visibility), do a tiny transform\n",
    "    df_text = spark.read.text(s3a_input)\n",
    "    _ = df_text.rdd.mapPartitions(_log_from_workers).count()\n",
    "\n",
    "    df_non_empty = df_text.filter(F.length(F.trim(\"value\")) > 0)\n",
    "    print(\"Non-empty line count:\", df_non_empty.count())\n",
    "\n",
    "    # 5) Optional write to S3\n",
    "    if s3a_output:\n",
    "        (df_non_empty\n",
    "            .withColumn(\"len\", F.length(\"value\"))\n",
    "            .withColumn(\"ts\", F.current_timestamp())\n",
    "            .write.mode(\"overwrite\").parquet(s3a_output))\n",
    "        print(f\"Wrote parquet to: {s3a_output}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "# Example:\n",
    "# main(\"s3://<BUCKET_NAME>/end-to-end/sample-input/test.txt\",\n",
    "#      \"s3://<BUCKET_NAME>/end-to-end/sample-output/\", make_synth=True, synth_lines=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e33f8-5422-4925-ac3a-58b435eba30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = os.environ['S3_BUCKET_NAME']\n",
    "s3_input_text=f\"s3://{s3_bucket}/end-to-end/sample-input/test.txt\"\n",
    "s3_output_parquet=f\"s3://{s3_bucket}/end-to-end/sample-output/\"\n",
    "#Delete if it exists. Not necessary because we use overwrite mode\n",
    "#delete_if_exists(s3_input_text)\n",
    "#delete_if_exists(s3_output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4201678-8a13-4a79-9d8a-53c793567d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(s3_input_text,s3_output_parquet,make_synth=True,synth_lines=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43138314-4e86-44e9-b995-fbbc423b353e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
